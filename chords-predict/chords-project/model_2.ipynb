{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victor.serra\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "import librosa\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchaudio.transforms import MelSpectrogram, MFCC, SpectralCentroid\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pacotes para o relatório de hardware\n",
    "import gc\n",
    "import types\n",
    "import pkg_resources\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Seed para reproduzir os mesmos resultados\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed(10)\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dos dados e cálculo do desvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = 'chords/variation_chord_audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labels.csv')\n",
    "labels['file_path'] = labels['file_name'].apply(lambda x: os.path.join(audio_folder, x))\n",
    "labels['chord_idx'] = pd.Categorical(labels['chord']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_dict = {\n",
    "    'mel_spectrogram': torchaudio.transforms.MelSpectrogram(sample_rate=22050),\n",
    "    'mfcc': torchaudio.transforms.MFCC(sample_rate=22050),\n",
    "    'spectral': torchaudio.transforms.SpectralCentroid(sample_rate=22050),\n",
    "    'chroma': lambda x: torch.tensor(librosa.feature.chroma_stft(y=x.numpy(), sr=22050)),\n",
    "    'tonnetz': lambda x: torch.tensor(librosa.feature.tonnetz(y=x.numpy(), sr=22050)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_statistics(audio_paths, feature_type):\n",
    "    assert feature_type in transforms_dict, f\"Feature {feature_type} is not supported.\"\n",
    "    \n",
    "    # Obtenha a transformação específica\n",
    "    transform = transforms_dict[feature_type]\n",
    "    \n",
    "    # Inicialize acumuladores\n",
    "    sum_of_features = None\n",
    "    sum_of_features_squared = None\n",
    "    num_features = 0\n",
    "    \n",
    "    # Loop sobre os caminhos de áudio\n",
    "    for path in tqdm(audio_paths):\n",
    "        # Carrega o áudio\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        # Converte para o domínio da frequência e aplica a transformação\n",
    "        if callable(transform):\n",
    "            feature = transform(waveform.squeeze(0))  # assumindo que waveform é um tensor 2D [channels, time]\n",
    "        else:\n",
    "            feature = transform(waveform)\n",
    "        \n",
    "        # Converte para tensor se ainda não for um\n",
    "        if not isinstance(feature, torch.Tensor):\n",
    "            feature = torch.from_numpy(np.array(feature))\n",
    "        \n",
    "        feature_mean = feature.mean(dim=-1)\n",
    "        feature_squared_mean = (feature ** 2).mean(dim=-1)\n",
    "        \n",
    "        # Inicializa os acumuladores se ainda não estiverem inicializados\n",
    "        if sum_of_features is None:\n",
    "            sum_of_features = torch.zeros_like(feature_mean)\n",
    "            sum_of_features_squared = torch.zeros_like(feature_mean)\n",
    "        \n",
    "        # Atualiza os acumuladores\n",
    "        sum_of_features += feature_mean\n",
    "        sum_of_features_squared += feature_squared_mean\n",
    "        num_features += 1\n",
    "    \n",
    "    # Calcula média e desvio padrão\n",
    "    mean = sum_of_features / num_features\n",
    "    std = (sum_of_features_squared / num_features - mean ** 2).sqrt()\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37131/37131 [09:01<00:00, 68.55it/s] \n"
     ]
    }
   ],
   "source": [
    "normMean, normStd = calculate_feature_statistics(labels['file_path'], 'mel_spectrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_modelo(num_classes, feature_extract, audio_feature_type='mel_spectrogram', use_pretrained=True):\n",
    "    input_size = 224  # Este é o tamanho padrão para o modelo Densenet\n",
    "\n",
    "    # Inicializa o número de canais de entrada com base na característica de áudio selecionada\n",
    "    feature_channels = {\n",
    "        'mel_spectrogram': 1,\n",
    "        'mfcc': 13,\n",
    "        'chroma': 12,\n",
    "        'spectral_centroid': 1,  \n",
    "        'tonnetz': 6\n",
    "    }\n",
    "\n",
    "    num_input_channels = feature_channels.get(audio_feature_type, 1)  # Padrão para espectrograma de mel\n",
    "\n",
    "    model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "    \n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "    model_ft.features.conv0 = nn.Conv2d(num_input_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    num_ftrs = model_ft.classifier.in_features\n",
    "    \n",
    "    model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, audio_feature_type='mel_spectrogram', transform=None):\n",
    "        self.df = df\n",
    "        self.audio_feature_type = audio_feature_type\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_path = self.df['file_path'][index]\n",
    "        label = torch.tensor(int(self.df['chord_idx'][index]))\n",
    "\n",
    "        if self.audio_feature_type in ['mel_spectrogram', 'mfcc']:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "            if self.audio_feature_type == 'mel_spectrogram':\n",
    "                feature = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(waveform)\n",
    "            elif self.audio_feature_type == 'mfcc':\n",
    "                feature = torchaudio.transforms.MFCC(sample_rate=sample_rate)(waveform)\n",
    "        elif self.audio_feature_type in ['chroma', 'tonnetz']:\n",
    "            y, sr = librosa.load(audio_path)\n",
    "            \n",
    "            if self.audio_feature_type == 'chroma':\n",
    "                feature = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            elif self.audio_feature_type == 'tonnetz':\n",
    "                feature = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "\n",
    "            # Converte a característica para um tensor PyTorch\n",
    "            feature = torch.tensor(feature).float()\n",
    "        else:\n",
    "            raise ValueError(f\"Feature type {self.audio_feature_type} not recognized.\")\n",
    "\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular erro em treino e validação durante o treinamento\n",
    "class CalculaMetricas(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para erro e acurácia em treino\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "# Função de treino do modelo\n",
    "def treina_modelo(treino_loader, model, criterion, optimizer, epoch):\n",
    "    \n",
    "    # Coloca o modelo em modo de treino\n",
    "    model.train()\n",
    "    \n",
    "    # Inicializa objetos de cálculo de métricas\n",
    "    train_loss = CalculaMetricas()\n",
    "    train_acc = CalculaMetricas()\n",
    "    \n",
    "    # Iteração\n",
    "    curr_iter = (epoch - 1) * len(treino_loader)\n",
    "    \n",
    "    # Loop de treino\n",
    "    for i, data in enumerate(treino_loader):\n",
    "        \n",
    "        # Extra os dados\n",
    "        images, labels = data\n",
    "        \n",
    "        # Tamanho da imagem\n",
    "        N = images.size(0)\n",
    "        \n",
    "        # Coloca imagens e labels no device\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        # Zera os gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Previsão do modelo\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Erro do modelo\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Obtem a previsão de maior probabilidade\n",
    "        prediction = outputs.max(1, keepdim = True)[1]\n",
    "        \n",
    "        # Atualiza as métricas\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        \n",
    "        # Iteração\n",
    "        curr_iter += 1\n",
    "        \n",
    "        # Print e update das métricas\n",
    "        # A condição *** and curr_iter < 1000 *** pode ser removida se você quiser treinar com o dataset completo\n",
    "        if (i + 1) % 100 == 0 and curr_iter < 1000:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (epoch, \n",
    "                                                                                       i + 1, \n",
    "                                                                                       len(treino_loader), \n",
    "                                                                                       train_loss.avg, \n",
    "                                                                                       train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "            \n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_val, total_acc_val = [],[]\n",
    "# Função para validação\n",
    "def valida_modelo(val_loader, model, criterion, optimizer, epoch):\n",
    "    \n",
    "    # Coloca o modelo em modo de validação\n",
    "    model.eval()\n",
    "    \n",
    "    # Inicializa objetos de cálculo de métricas\n",
    "    val_loss = CalculaMetricas()\n",
    "    val_acc = CalculaMetricas()\n",
    "    \n",
    "    # Validação\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            \n",
    "            images, labels = data\n",
    "            \n",
    "            N = images.size(0)\n",
    "            \n",
    "            images = Variable(images).to(device)\n",
    "            \n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            prediction = outputs.max(1, keepdim = True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels['chord_idx']\n",
    "_, df_validacao = train_test_split(labels, test_size = 0.2, random_state = 101, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7427, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validacao.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chord_idx\n",
       "186    45\n",
       "20     44\n",
       "303    43\n",
       "109    42\n",
       "334    42\n",
       "       ..\n",
       "92      6\n",
       "197     6\n",
       "181     6\n",
       "227     6\n",
       "120     6\n",
       "Name: count, Length: 373, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validacao['chord_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta função identifica se uma imagem faz parte do conjunto train ou val\n",
    "def get_val_rows(x):\n",
    "    val_list = list(df_validacao['clean'])\n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica treino ou validação\n",
    "labels['train_or_val'] = labels['clean']\n",
    "labels['train_or_val'] = labels['train_or_val'].apply(get_val_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra as linhas de treino\n",
    "df_treino = labels[labels['train_or_val'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28783\n",
      "7427\n"
     ]
    }
   ],
   "source": [
    "print(len(df_treino))\n",
    "print(len(df_validacao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chord_idx\n",
       "186    175\n",
       "20     172\n",
       "303    166\n",
       "334    165\n",
       "109    165\n",
       "      ... \n",
       "169     26\n",
       "227     25\n",
       "181     25\n",
       "120     24\n",
       "197     22\n",
       "Name: count, Length: 373, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_treino['chord_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chord\n",
       "D#m11    45\n",
       "A#m11    44\n",
       "Fm11     43\n",
       "C#m11    42\n",
       "G#m11    42\n",
       "         ..\n",
       "C#5       6\n",
       "D5        6\n",
       "D#aug     6\n",
       "E5        6\n",
       "C5        6\n",
       "Name: count, Length: 373, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validacao['chord'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos dividir o conjunto de validação em um conjunto de validação e um conjunto de teste\n",
    "df_validacao, df_teste = train_test_split(df_validacao, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset do índice\n",
    "df_validacao = df_validacao.reset_index()\n",
    "df_teste = df_teste.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validacao.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo que será treinado\n",
    "# nome_modelo = 'densenet'\n",
    "# nome_modelo = 'resnet'\n",
    "# nome_modelo = 'inception'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos treinar o modelo e sempre atualizar os pesos\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o modelo\n",
    "model_ft, input_size = inicializa_modelo(num_classes, feature_extract, use_pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloca o modelo no device\n",
    "model = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample_rate = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_treino = transforms.Compose([\n",
    "    T.Resample(orig_freq=22050, new_freq=3500),  # sr é a taxa de amostragem original do áudio\n",
    "    T.TimeStretch(),  # Para mudar a velocidade do áudio\n",
    "    T.PitchShift(n_steps=30, n_fft=2048, sample_rate=target_sample_rate),  # Para mudar o pitch\n",
    "    T.FrequencyMasking(freq_mask_param=15),  # Para adicionar máscaras de frequência\n",
    "    T.TimeMasking(time_mask_param=35),  # Para adicionar máscaras de tempo\n",
    "    # ... outras transformações específicas de áudio\n",
    "    T.AmplitudeToDB(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformações das imagens de validação\n",
    "transform_val = transforms.Compose([\n",
    "    #T.Resample(orig_freq=sr, new_freq=target_sample_rate),\n",
    "    T.AmplitudeToDB(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organiza e transforma os dados de treino\n",
    "set_treino = AudioDataset(df_treino, transform = transform_treino)\n",
    "loader_treino = DataLoader(set_treino, batch_size = 32, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo em validação\n",
    "set_val = AudioDataset(df_validacao, transform = transform_val)\n",
    "loader_val = DataLoader(set_val, batch_size = 32, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo em teste\n",
    "set_teste = AudioDataset(df_teste, transform = transform_val)\n",
    "loader_teste = DataLoader(set_teste, batch_size = 32, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos o otimizador Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos cross entropy loss como função de perda\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "epoch_num = 3\n",
    "best_val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    # Execute a função de treino\n",
    "    loss_train, acc_train = treina_modelo(loader_treino, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # Executa a função de validação\n",
    "    loss_val, acc_val = valida_modelo(loader_val, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # Calcula as métricas\n",
    "    total_loss_val.append(loss_val)\n",
    "    total_acc_val.append(acc_val)\n",
    "    \n",
    "    # Verifica a acurácia em validação\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        print('*****************************************************')\n",
    "        print('Melhor Resultado: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "        print('*****************************************************')\n",
    "\n",
    "        torch.save(model.state_dict(), f'resnet_model_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
