{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time as time\n",
    "import random\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from config import new_chromedriver_path, user_agents, path, connection_string\n",
    "from web_scraper import WebScraper\n",
    "from vpn import VPNChanger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_sites = pd.read_csv('C:/Users/victo/AI/Cognificadora/cognificar_leiloes/lista_sites.csv', sep=';')\n",
    "lista_sites = lista_sites['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sodero = pd.read_csv('df_sodero.csv')\n",
    "df_lista = df_sodero['Leilao'].apply(lambda x: urljoin(lista_sites[0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WebScraper(new_chromedriver_path, user_agents, path)\n",
    "# vpn_changer = VPNChanger()\n",
    "# vpn_files = vpn_changer.get_vpn_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('bens.csv'):\n",
    "    os.remove('bens.csv')\n",
    "if os.path.exists('infos_especificas.csv'):\n",
    "    os.remove('infos_especificas.csv')\n",
    "if os.path.exists('df_intermediario.csv'):\n",
    "    os.remove('df_intermediario.csv')\n",
    "if os.path.exists('df_leilao_aberto.csv'):\n",
    "    os.remove('df_leilao_aberto.csv')\n",
    "if os.path.exists('df_leilao_encerrado.csv'):\n",
    "    os.remove('df_leilao_encerrado.csv')\n",
    "if os.path.exists('df_concat.csv'):\n",
    "    os.remove('df_concat.csv')\n",
    "if os.path.exists('control_home.txt'):\n",
    "    os.remove('control_home.txt')\n",
    "if  os.path.exists('control_pag.txt'):\n",
    "    os.remove('control_pag.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_aberto(lista_bens):\n",
    "    df_intermediario = pd.DataFrame()\n",
    "    infos_base = []\n",
    "    for inf in lista_bens:\n",
    "        titulo = inf.find('h2', class_='titulo')\n",
    "        titulo = titulo.text if titulo else None\n",
    "\n",
    "        link = inf.find('a', class_='link')\n",
    "        link = link.get('href') if link else None\n",
    "\n",
    "        imagem = inf.find('img', class_='imagemDisponivel img-responsive')\n",
    "        imagem = imagem.get('src') if imagem else None\n",
    "\n",
    "        status_lote = inf.find('div', class_='statusLote statusLote-Aguardando')\n",
    "        status_lote = status_lote.text if status_lote else None\n",
    "\n",
    "        lance_atual = inf.find('span', class_='lance-atual')\n",
    "        lance_atual = lance_atual.get('title') if lance_atual else None\n",
    "\n",
    "        visitas_lances = inf.find('span', class_='visitas-lances')\n",
    "        visitas_lances = visitas_lances.text if visitas_lances else None\n",
    "\n",
    "        infos_base.append(\n",
    "            {\n",
    "                'Titulo': titulo,\n",
    "                'Imagem': imagem,\n",
    "                'Link': link,\n",
    "                'Status Lote': status_lote,\n",
    "                'Lance Atual': lance_atual,\n",
    "                \n",
    "                'Visitas_lances': visitas_lances\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        df_bens = pd.DataFrame(infos_base).replace(r'\\n|\\t', '', regex=True)\n",
    "        df_bens.to_csv('bens.csv', index=False)\n",
    "    if os.path.exists('df_intermediario.csv'):\n",
    "        df_intermediario = pd.read_csv('df_intermediario.csv')\n",
    "    df_intermediario = pd.concat([df_intermediario, df_bens], axis=0)\n",
    "    df_intermediario.to_csv('df_intermediario.csv', index=False)\n",
    "\n",
    "    return df_intermediario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "def find_chrome_processes():\n",
    "    chrome_processes = []\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        if proc.info['name'] == 'chrome.exe':\n",
    "            chrome_processes.append(proc)\n",
    "    return chrome_processes\n",
    "\n",
    "def kill_chrome_processes():\n",
    "    chrome_processes = find_chrome_processes()\n",
    "    for process in chrome_processes:\n",
    "        try:\n",
    "            subprocess.run(['taskkill', '/F', '/PID', str(process.pid)], check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            pass\n",
    "\n",
    "kill_chrome_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = scraper.get_current_index(\"control_home\")\n",
    "# vpn_changer.connect_vpn(vpn_files[random.randint(0, len(vpn_files) - 1)])\n",
    "\n",
    "df_leilao_aberto = pd.DataFrame()\n",
    "try:\n",
    "    for x in range(start_index, len(df_lista)):\n",
    "        scraper.driver = scraper.header_configuration(headless=True)   \n",
    "        time.sleep(random.randint(13,16))\n",
    "        f = df_lista.iloc[x]\n",
    "        \n",
    "        # if x % 5 == 0 and x > 0:\n",
    "        #    vpn_changer.change_ip()\n",
    "\n",
    "        scraper.driver.get(f'{f}ordenacao/nu_lote/tipo-ordenacao/crescente/qtde-itens/{random.randint(4000,6000)}/')\n",
    "        scraper.driver.delete_all_cookies()\n",
    "\n",
    "        print(scraper.driver.current_url)\n",
    "        if \"validate.perfdrive.com\" in urlparse(scraper.driver.current_url).netloc:\n",
    "            scraper.driver = scraper.scrape_with_captcha_retry(scraper.driver.current_url, f'{f}ordenacao/nu_lote/tipo-ordenacao/crescente/qtde-itens/{random.randint(4000,6000)}/') #, df_lista\n",
    "        if 'encerrado' in scraper.driver.current_url:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(scraper.driver.page_source, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            span = soup.find('span', class_='textoAux2')\n",
    "            if span.text == 'Leilão não encontrado!':\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ul_paginas = soup.find('ul', class_='tipo-vizualizacao visual_imagemlista')\n",
    "        lista_bens = ul_paginas.find_all('li', attrs={'guardar': True})\n",
    "\n",
    "        df_aberto = scrape_aberto(lista_bens)\n",
    "        if os.path.exists('df_leilao_aberto.csv'):\n",
    "            df_leilao_aberto = pd.read_csv('df_leilao_aberto.csv')\n",
    "        df_leilao_aberto = pd.concat([df_leilao_aberto, df_aberto], axis=0)\n",
    "        df_leilao_aberto.drop_duplicates().to_csv('df_leilao_aberto.csv', index=False)\n",
    "\n",
    "        print(f\"Index {x + 1} of {len(df_lista)}\")\n",
    "        scraper.update_current_index(x + 1, \"control_home\")\n",
    "        scraper.driver.quit()\n",
    "        \n",
    "        # scraper.kill_chrome_processes()\n",
    "        scraper.delete_cache(get_previous_url=False)\n",
    "        \n",
    "    pd_ajuste = pd.read_csv('df_leilao_aberto.csv')\n",
    "    pd_ajuste['link_clicavel'] = pd_ajuste['Link'].apply(lambda x: urljoin(lista_sites[0], x))\n",
    "    pd_ajuste['data_hora_atualizacao'] = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    pd_ajuste.to_csv('df_leilao_aberto.csv', index=False)\n",
    "    # vpn_changer.disconnect_vpn()\n",
    "    print(f'Finalizado as {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "finally:\n",
    "    try:\n",
    "        # vpn_changer.disconnect_vpn()\n",
    "        scraper.driver.service.process.terminate()\n",
    "        scraper.driver.quit()\n",
    "        scraper.delete_cache(get_previous_url=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviar para o blob do azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "blob_client = blob_service_client.get_blob_client(\"leilao-sodre\", \"df_leilao_aberto\")\n",
    "\n",
    "with open('df_leilao_aberto.csv', \"rb\") as data:\n",
    "    blob_client.upload_blob(data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "scraper.driver = scraper.header_configuration(headless=False)\n",
    "scraper.driver.get('https://www.superbid.net/todos-eventos?filter=modalityId:[1,4];subMarketplaces.id:all&byPage=marketPlacePage&pageNumber=1&pageSize=100000')\n",
    "\n",
    "soup = BeautifulSoup(scraper.driver.page_source, 'html.parser')\n",
    "time.sleep(30)\n",
    "# WebDriverWait(scraper.driver, 30).until(EC._element_if_visible((By.XPATH, \"//div[@class='MuiGrid-root MuiGrid-container jss93 css-1hk9k3n']\")))\n",
    "div_geral = soup.find_all('div', class_='MuiGrid-root MuiGrid-item MuiGrid-grid-xs-auto css-1gd6ahq')\n",
    "\n",
    "informacao_superbid = []\n",
    "for div in div_geral:\n",
    "    # get the information from personalized attribute data-auction-name\n",
    "    auction_name = div['data-auction-name']\n",
    "\n",
    "    informacao_superbid.append(\n",
    "        {\n",
    "            'leilao': auction_name\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_superbid = pd.DataFrame(informacao_superbid)\n",
    "df_superbid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_geral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
