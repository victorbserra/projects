{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time   as time\n",
    "from bs4          import BeautifulSoup\n",
    "from datetime     import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from config       import new_chromedriver_path, user_agents, path, connection_string\n",
    "from web_scraper  import WebScraper\n",
    "from vpn          import VPNChanger\n",
    "from selenium.webdriver.support.ui  import WebDriverWait\n",
    "from selenium.webdriver.support     import expected_conditions as EC\n",
    "from selenium.webdriver.common.by   import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing links from Milano regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper = WebScraper('chromedriver-win64/chromedriver.exe',user_agents, path)\n",
    "# driver = scraper.header_configuration(headless=False)\n",
    "# driver.get(f'https://www.immobiliare.it/pt/vendita-case/milano/')\n",
    "# html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# neighborhood_html = html.find_all('a', class_='in-breadcrumbLink__dropdownLink')\n",
    "\n",
    "# neighborhood_list = []\n",
    "# for n in neighborhood_html:\n",
    "#     data = {\n",
    "#         'neighborhood': n.text.strip(),\n",
    "#         'link': n.get('href')\n",
    "#     }\n",
    "\n",
    "#     neighborhood_list.append(data)\n",
    "    \n",
    "# df_neighborhood = pd.DataFrame(neighborhood_list)\n",
    "\n",
    "# df_neighborhood.to_csv('neighborhood.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_neighborhood_list = []\n",
    "\n",
    "# for i in range(len(df_neighborhood)):\n",
    "#     driver.get(df_neighborhood['link'].iloc[i]) \n",
    "\n",
    "#     try:\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.nd-list.nd-list--arrow.in-breadcrumb'))\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao carregar a página: {e}\")\n",
    "#         continue\n",
    "\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#     ul_neighborhood = soup.find('ul', class_='nd-list nd-list--arrow in-breadcrumb')\n",
    "    \n",
    "#     if ul_neighborhood:\n",
    "#         li = ul_neighborhood.find_all('li', class_='nd-list__item')\n",
    "#         drill_down_list = li[-1].find_all('a', class_='in-breadcrumbLink__dropdownLink')\n",
    "\n",
    "#         for d in drill_down_list:\n",
    "#             data = {\n",
    "#                 'neighborhood': df_neighborhood['neighborhood'].iloc[i],\n",
    "#                 'drill_down': d.text.strip(),\n",
    "#                 'link': d.get('href')\n",
    "#             }\n",
    "\n",
    "#             complete_neighborhood_list.append(data)\n",
    "\n",
    "#     # Aguarde 1 segundo entre as requisições\n",
    "#     time.sleep(1)\n",
    "\n",
    "# df_complete_neighborhood = pd.DataFrame(complete_neighborhood_list)\n",
    "# df_complete_neighborhood.to_csv('complete_neighborhood.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad scrap by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_neighborhood = pd.read_csv(\"complete_neighborhood.csv\")\n",
    "df_complete_neighborhood.link = df_complete_neighborhood.link.str.replace('vendita-case','affitto-case')\n",
    "df_complete_neighborhood.link = df_complete_neighborhood.link.str.replace('pt/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WebScraper('chromedriver-win64/chromedriver.exe',user_agents, path)\n",
    "driver = scraper.header_configuration(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_neighborhood.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'output_rent'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.get(df_complete_neighborhood.link.iloc[0])\n",
    "\n",
    "# html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# ul = html.find('ul', class_='nd-list in-searchLayoutList ls-results')\n",
    "# li = ul.find_all('li', class_='nd-list__item in-searchLayoutListItem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "r = 1  # Contador de região\n",
    "# Iterando por todos os links do df_complete_neighborhood\n",
    "for idx in range(len(df_complete_neighborhood)):\n",
    "    driver.get(df_complete_neighborhood.link.iloc[idx])\n",
    "\n",
    "    ### Numerar última página\n",
    "    html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    ul = html.find('ul', class_='nd-list in-searchLayoutList ls-results') if html else None\n",
    "    if not ul:\n",
    "        continue\n",
    "    li = ul.find_all('li', class_='nd-list__item in-searchLayoutListItem') if ul else None\n",
    "    if not li:\n",
    "        continue\n",
    "    div_pagination__list = html.find_all('div', class_='in-pagination__list')\n",
    "\n",
    "    if div_pagination__list:\n",
    "        for div in div_pagination__list:\n",
    "            # Procura o número da última página\n",
    "            pagination_items = div.find_all('div', class_='nd-button nd-button--ghost is-disabled in-pagination__item is-mobileHidden')\n",
    "\n",
    "            if pagination_items:\n",
    "                last_page = int(pagination_items[-1].text.strip())\n",
    "            else:\n",
    "                last_page = int(div_pagination__list[0].find_all('a')[-1].text.strip())\n",
    "    else:\n",
    "        last_page = 1  # Se não houver paginação, considera apenas a primeira página\n",
    "    ######### Fim da Numeração\n",
    "\n",
    "    \n",
    "    print(f'Iniciando scraping do link {df_complete_neighborhood.drill_down.iloc[idx]}')\n",
    "    print(f'{r}/{len(df_complete_neighborhood)}')\n",
    "    print('#################################')\n",
    "    \n",
    "    intermediary_list = []\n",
    "\n",
    "    #### Início do scraping dessa região\n",
    "    for i in range(1, last_page + 1):  # Itera até a última página (incluindo last_page)\n",
    "        if i == 1:\n",
    "            print(f'Página {i}/{last_page} do link {df_complete_neighborhood.link.iloc[idx]}')\n",
    "        else:\n",
    "            print(f'Página {i}/{last_page} do link {next_page}')\n",
    "\n",
    "        # Carrega o HTML e faz o parsing\n",
    "        html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        ul = html.find('ul', class_='nd-list in-searchLayoutList ls-results') if html else None\n",
    "        if not ul:\n",
    "            break\n",
    "        li = ul.find_all('li', class_='nd-list__item in-searchLayoutListItem') if ul else None\n",
    "        if not li:\n",
    "            break\n",
    "\n",
    "        for l in li:\n",
    "            div_pagination_control = html.find_all('div', class_='in-pagination__control')\n",
    "\n",
    "            # Captura os dados do projeto\n",
    "            project_id = l.find('div', class_='in-listingCard').get('id')\n",
    "            link = l.find('a').get('href')\n",
    "            ads_title = l.find('a').get('title')\n",
    "\n",
    "            # Verifica se há múltiplas unidades (div.nd-stripe__item)\n",
    "            listing_card_units = l.find('div', class_='nd-stripe__item in-listingCardUnits__item')\n",
    "            if listing_card_units:\n",
    "                for card in l.find_all('div', class_='nd-stripe__item in-listingCardUnits__item'):\n",
    "                    image = card.find('img')\n",
    "                    image = image.get('src') if image else None\n",
    "                    title = card.find('div', class_='in-listingCardUnit__title')\n",
    "                    title = title.text.strip() if title else None\n",
    "                    price = card.find('span', class_='in-listingCardUnit__price')\n",
    "                    price = price.text.strip().replace('€', '').replace('\\xa0', '').replace('.', '').strip() if price else None\n",
    "\n",
    "                    surface, bathrooms, floor, rooms = None, None, None, None\n",
    "                    features = card.find_all('div', class_='in-listingFeatures__item')\n",
    "                    if features:\n",
    "                        for f in features:\n",
    "                            if 'data-type' in f.attrs:\n",
    "                                if f['data-type'] == 'surface':\n",
    "                                    surface = f.text.strip()\n",
    "                                elif f['data-type'] == 'bathrooms':\n",
    "                                    bathrooms = f.text.strip()\n",
    "                                elif f['data-type'] == 'floor':\n",
    "                                    floor = f.text.strip()\n",
    "                                elif f['data-type'] == 'rooms':\n",
    "                                    rooms = f.text.strip()\n",
    "\n",
    "                    data = {\n",
    "                        'project_id': project_id,\n",
    "                        'ads_title': ads_title,\n",
    "                        'link': link,\n",
    "                        'image': image,\n",
    "                        'title': title,\n",
    "                        'price': price,\n",
    "                        'surface': surface,\n",
    "                        'bathrooms': bathrooms,\n",
    "                        'floor': floor,\n",
    "                        'rooms': rooms,\n",
    "                        'page': i,\n",
    "                        'neighborhood_link': df_complete_neighborhood.link.iloc[idx],\n",
    "                        'neighborhood': df_complete_neighborhood.neighborhood.iloc[idx],\n",
    "                        'drill_down': df_complete_neighborhood.drill_down.iloc[idx]\n",
    "                    }\n",
    "                    data_list.append(data)\n",
    "                    intermediary_list.append(data)\n",
    "\n",
    "            else:\n",
    "                # Captura os dados do projeto se não houver múltiplas unidades\n",
    "                image = l.find('figure', class_='nd-figure nd-ratio nd-ratio--standard in-listingPhotos in-listingCardPropertyMediaMosaic__item')\n",
    "                image = image.img.get('src') if image else None\n",
    "                title = None\n",
    "                price = l.find('div', class_='in-listingCardPrice')\n",
    "                price = price.text.strip().replace('€', '').replace('\\xa0', '').replace('.', '').strip() if price else None\n",
    "                surface, bathrooms, floor, rooms = None, None, None, None\n",
    "                features = l.find_all('div', class_='in-listingCardFeatureList__item')[:4]\n",
    "\n",
    "                if features:\n",
    "                    for f in features:\n",
    "                        use_tag = f.find('use')\n",
    "                        if use_tag and 'xlink:href' in use_tag.attrs:\n",
    "                            href = use_tag.get('xlink:href')\n",
    "                            if href == '#size':\n",
    "                                surface = f.text.strip()\n",
    "                            elif href == '#bath':\n",
    "                                bathrooms = f.text.strip()\n",
    "                            elif href == '#stairs':\n",
    "                                floor = f.text.strip()\n",
    "                            elif href == '#planimetry':\n",
    "                                rooms = f.text.strip()\n",
    "\n",
    "                data = {\n",
    "                    'project_id': project_id,\n",
    "                    'ads_title': ads_title,\n",
    "                    'link': link,\n",
    "                    'image': image,\n",
    "                    'title': title,\n",
    "                    'price': price,\n",
    "                    'surface': surface,\n",
    "                    'bathrooms': bathrooms,\n",
    "                    'floor': floor,\n",
    "                    'rooms': rooms,\n",
    "                    'page': i,\n",
    "                    'neighborhood_link': df_complete_neighborhood.link.iloc[idx],\n",
    "                    'neighborhood': df_complete_neighborhood.neighborhood.iloc[idx],\n",
    "                    'drill_down': df_complete_neighborhood.drill_down.iloc[idx]\n",
    "                }\n",
    "                data_list.append(data)\n",
    "                intermediary_list.append(data)\n",
    "\n",
    "        # Verifica se estamos na última página antes de tentar a próxima\n",
    "        try:\n",
    "            if i < last_page:\n",
    "                next_page = fr'{df_complete_neighborhood.link.iloc[idx]}?pag={i+1}'\n",
    "                time.sleep(2)  # Pausa para evitar sobrecarregar o servidor\n",
    "                driver.get(next_page)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.nd-list.in-searchLayoutList.ls-results'))\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    print('FINALIZADO\\n#################################')\n",
    "    r += 1\n",
    "    # Salva intermediários a cada página\n",
    "    df_intermediate = pd.DataFrame(intermediary_list)\n",
    "    df_intermediate.to_csv(f'{output_folder}/intermediate_microzone_{df_complete_neighborhood.drill_down.iloc[idx]}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Gera o DataFrame principal e salva\n",
    "df_complete = pd.DataFrame(data_list)\n",
    "df_complete.to_csv(f'{output_folder}/complete_neighborhood_data.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Scraping finalizado e dados salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime               import datetime\n",
    "from google.cloud           import bigquery\n",
    "from google.oauth2          import service_account\n",
    "from google.cloud.bigquery  import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file('methodius-advisory-86691f61031e.json')\n",
    "client = bigquery.Client(credentials=credentials, project='methodius-advisory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sql_file, var):  \n",
    "    if var == True:\n",
    "        dados = client.query(sql_file)\n",
    "        result = dados.result()\n",
    "        df = result.to_dataframe()\n",
    "        return df\n",
    "    else:\n",
    "        with open(sql_file, 'r') as f:\n",
    "            sql = f.read()\n",
    "        dados = client.query(sql)\n",
    "        result = dados.result()\n",
    "        df = result.to_dataframe()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rows = 1048576\n",
    "\n",
    "def save_to_multiple_sheets(df, writer, sheet_name_prefix):\n",
    "    num_chunks = -(-len(df) // (max_rows - 1))\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * (max_rows - 1)\n",
    "        end_idx = (i + 1) * (max_rows - 1)\n",
    "        df_chunk = df.iloc[start_idx:end_idx]\n",
    "        df_chunk.to_excel(writer, sheet_name=f\"{sheet_name_prefix}_{i+1}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complete = pd.read_csv('output_rent/complete_neighborhood_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = {\n",
    "    'project_id': int,\n",
    "    'ads_title': str,\n",
    "    'link': str,\n",
    "    'image': str,\n",
    "    'title': str,\n",
    "    'price': str,\n",
    "    'surface': str,\n",
    "    'bathrooms': str,\n",
    "    'floor': str,\n",
    "    'rooms': str,\n",
    "    'page': int,\n",
    "    'neighborhood_link': str,\n",
    "    'neighborhood': str,\n",
    "    'drill_down': str\n",
    "}\n",
    "\n",
    "df_complete = df_complete.astype(type)\n",
    "df_complete['datetime_update'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'methodius-advisory'\n",
    "dataset = 'dw1_rawtables'\n",
    "table_name = 'raw_rent_immobiliare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref = client.dataset(dataset).table(table_name)\n",
    "table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(TableReference(DatasetReference('methodius-advisory', 'dw1_rawtables'), 'raw_rent_immobiliare'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    bigquery.SchemaField('project_id', 'INT64', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('ads_title', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('link', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('image', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('title', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('price', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('surface', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('bathrooms', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('floor', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('rooms', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('page', 'INT64', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('neighborhood_link', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('neighborhood', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('drill_down', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('datetime_update', 'DATETIME', mode='NULLABLE')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref = client.dataset(dataset).table(table_name)\n",
    "try:\n",
    "    table = client.get_table(table_ref)\n",
    "    print('A tabela já existe. Os novos dados serão carregados na tabela.')\n",
    "except:\n",
    "    print('A tabela não existe. Será criada agora.')\n",
    "    table = bigquery.Table(table_ref)\n",
    "    table = client.create_table(table)\n",
    "    print('A tabela foi criada com sucesso.')\n",
    "\n",
    "# Carrega os novos dados na tabela existente\n",
    "job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND', schema=schema)\n",
    "job = client.load_table_from_dataframe(df_complete.drop_duplicates(), table_ref, job_config=job_config)\n",
    "job.result()\n",
    "print('Os novos dados foram carregados com sucesso na tabela existente.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
